
### First pass papers
This file contains an overview of the papers I recently red (only superficially in one pass). Sorted by time of reading. (✓✓) only given if the observed parts of the paper are extremely well written.

#### `2018-10-01`  Video summarization (revisit)
##### Notes
- video summarization := "compress" video by extracting important parts
  - viewpoint := yields `important` aspects of the image / video
  - applying the video summarization we receive similarity metrics wrt the
    viewpoint.
  - not one optimal viewpoint

##### Q
- contribution meta:
  - viewpoint specified as `similarity` (in a semantic sense) which, once
    again is depending on the aspect that is considered important.
  - focuses on video-level similarities (supervized) 
- core contribution:
  1. Proposes a novel video summarization method from multiple groups; inspired
     by Fisher's discriminant 
  2. Introduces a dataset for that purpose
  3. Developed optimization algorithm t
- usefulness:
  - can be used for feature extraction
- clarity: ✓✓
- correctness: ✓

```bibtex
@inproceedings{kanehira2018aware,
  title={aware Video Summarization},
  author={Kanehira, Atsushi and Van Gool, Luc and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7435--7444},
  year={2018}
}
```


#### `2018-10-01` Gaze Anticipation (revisit)
##### Notes
- point of gaze := point which human is fixating
- method (called Deep Future Gaze := `DFG`): 
  - GAN
    - GENERATOR: 
        1. (PREDICT FRAMES) Two-stream spatial temporal convolutional net 
           (3d-CNN); one for background; one for foreground 
        2. Second 3d-CNN employed for gaze anticipations on top 
    - DISCRIMINATOR
    - This GAN is used for predicting the future frames
- [Experimental results and code](https://github.com/Mengmi/deepfuturegaze_gan    )

##### Q
- contribution 
  1. Aforementioned method outperforms state-of-the-art methods
    - prediction of the point of gaze in the future (order of a few seconds)
  2. Introduces new egocentric dataset 
- reproducibility: ✓✓
- clarity: ✓
- correctness: ✓ even though I am extremely surprised that the prediction of the future frames in the next few seconds can be done reliably (gaze anticipation is stacked on top of that)

```latex
@inproceedings{zhang2017deep,
  title={Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks.},
  author={Zhang, Mengmi and Ma, Keng Teck and Lim, Joo-Hwee and Zhao, Qi and Feng, Jiashi},
  booktitle={CVPR},
  pages={3539--3548},
  year={2017}
}
```
